# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Q_7Vu2chVv2Yj3JFunAweZQqhgiqMB6
"""

from huggingface_hub import login
login("")

# Colab-ready AI Jeeven QLoRA Trainer with Auto-Resume
# ----------------------------------------------------

# 1Ô∏è‚É£ Install dependencies
!pip install transformers datasets accelerate peft bitsandbytes

# 2Ô∏è‚É£ Mount Google Drive for persistent storage
from google.colab import drive
drive.mount('/content/drive')

# 3Ô∏è‚É£ Imports
import os
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ---------------- CONFIG ----------------
MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"
DATA_PATH = "/content/drive/MyDrive/whatsapp_train.jsonl"  # Adjust path
OUT_DIR = "/content/drive/MyDrive/ai_jeeven_lora_2"
USE_4BIT = True

# ---------------- MAIN FUNCTION ----------------
def main():
    # Load dataset
    ds = load_dataset("json", data_files=DATA_PATH, split="train")

    # Tokenizer
    tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # Tokenization + labels
    def tok_fn(examples):
        tokens = tok(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=384
        )
        tokens["labels"] = tokens["input_ids"].copy()
        return tokens

    ds_tok = ds.map(tok_fn, batched=True, remove_columns=ds.column_names)

    # 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=USE_4BIT,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_quant_type="nf4",
    )

    # Load base model
    base = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        offload_folder="/content/offload",
        trust_remote_code=True
    )
    base = prepare_model_for_kbit_training(base)

    # LoRA config
    lora_cfg = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                        "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(base, lora_cfg)

    # Training arguments
    args = TrainingArguments(
        output_dir=OUT_DIR,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_train_epochs=1,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=50,
        save_steps=500,
        save_total_limit=5,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=ds_tok,
        tokenizer=tok,
    )

    # Automatically find latest checkpoint to resume
    checkpoint_dir = None
    if os.path.exists(OUT_DIR):
        checkpoints = [os.path.join(OUT_DIR, d) for d in os.listdir(OUT_DIR) if d.startswith("checkpoint")]
        if checkpoints:
            checkpoint_dir = max(checkpoints, key=os.path.getctime)
            print(f"üîÑ Resuming from checkpoint: {checkpoint_dir}")
        else:
            print("üÜï No checkpoint found. Starting from scratch.")
    else:
        os.makedirs(OUT_DIR, exist_ok=True)
        print("üÜï Output directory created. Starting from scratch.")

    # Train (resume if checkpoint exists)
    trainer.train(resume_from_checkpoint=checkpoint_dir)

    # Save final model & tokenizer
    trainer.save_model(OUT_DIR)
    tok.save_pretrained(OUT_DIR)
    print("‚úÖ Training complete. LoRA adapter saved at", OUT_DIR)

if __name__ == "__main__":
    main()

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel

MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"
ADAPTER_DIR = "/content/drive/MyDrive/ai_jeeven_lora_2"

# Load tokenizer
tok = AutoTokenizer.from_pretrained(MODEL_ID)

# Load base model
base = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    load_in_4bit=True,
    llm_int8_enable_fp32_cpu_offload=True
)

# Attach your LoRA adapter
model = PeftModel.from_pretrained(base, ADAPTER_DIR)

# Create pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tok)

print("üí¨ Chat with AI Jeeven (type 'quit' to exit)\n")

# Interactive loop
history = ""
while True:
    user_input = input("You: ")
    if user_input.lower() in ["quit", "exit"]:
        break

    # Append user message
    history += f"You: {user_input}\nAI Jeeven:"

    # Generate response
    out = pipe(history, max_new_tokens=150, do_sample=True, top_p=0.9, temperature=0.7)
    generated = out[0]["generated_text"]

    # üîé Extract only Jeeven‚Äôs reply (first block after "AI Jeeven:")
    if "AI Jeeven:" in generated:
        response = generated.split("AI Jeeven:")[-1].strip()
        # Stop if model starts writing another role (e.g., "You:" or "Lauren:")
        for stop_token in ["You:", "Lauren:", "Friend:"]:
            if stop_token in response:
                response = response.split(stop_token)[0].strip()
    else:
        response = generated.strip()

    # Print filtered response
    print(f"AI Jeeven: {response}\n")

    # Update history
    history += f" {response}\n"